{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, InputLayer, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_text</th>\n",
       "      <th>source</th>\n",
       "      <th>pornografi</th>\n",
       "      <th>sara</th>\n",
       "      <th>radikalisme</th>\n",
       "      <th>pencemaran_nama_baik</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stopword_tokenized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bukan cm spanduk prof video orasi mereka buku ...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['bukan', 'cm', 'spanduk', 'prof', 'video', 'o...</td>\n",
       "      <td>['cm', 'spanduk', 'prof', 'video', 'orasi', 'b...</td>\n",
       "      <td>cm spanduk prof video orasi buku dll udh sngat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>memeqbeceq gy sangegatel yh tetekmemeky drnjng...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['memeqbeceq', 'gy', 'sangegatel', 'yh', 'tete...</td>\n",
       "      <td>['memeqbeceq', 'gy', 'sangegatel', 'yh', 'tete...</td>\n",
       "      <td>memeqbeceq gy sangegatel yh tetekmemeky drnjng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pertama kali denger lagunya enk banget in dan ...</td>\n",
       "      <td>instagram</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['pertama', 'kali', 'denger', 'lagunya', 'enk'...</td>\n",
       "      <td>['kali', 'denger', 'lagunya', 'enk', 'banget',...</td>\n",
       "      <td>kali denger lagunya enk banget in pngn praktek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>astajim ini pasti yang kasih penghargaan ke ib...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['astajim', 'ini', 'pasti', 'yang', 'kasih', '...</td>\n",
       "      <td>['astajim', 'kasih', 'penghargaan', 'sri', 'an...</td>\n",
       "      <td>astajim kasih penghargaan sri antek aseng wahy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>beda kalau disini kalau komplain lgs di bully ...</td>\n",
       "      <td>kaskus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['beda', 'kalau', 'disini', 'kalau', 'komplain...</td>\n",
       "      <td>['beda', 'komplain', 'lgs', 'bully', 'ama', 'q...</td>\n",
       "      <td>beda komplain lgs bully ama quotgenkquot kl fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      original_text     source  \\\n",
       "0           0  bukan cm spanduk prof video orasi mereka buku ...    twitter   \n",
       "1           1  memeqbeceq gy sangegatel yh tetekmemeky drnjng...    twitter   \n",
       "2           2  pertama kali denger lagunya enk banget in dan ...  instagram   \n",
       "3           3  astajim ini pasti yang kasih penghargaan ke ib...     kaskus   \n",
       "4           4  beda kalau disini kalau komplain lgs di bully ...     kaskus   \n",
       "\n",
       "   pornografi  sara  radikalisme  pencemaran_nama_baik  \\\n",
       "0           0     0            1                     0   \n",
       "1           1     0            0                     0   \n",
       "2           0     0            0                     0   \n",
       "3           0     0            0                     0   \n",
       "4           0     0            0                     0   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  ['bukan', 'cm', 'spanduk', 'prof', 'video', 'o...   \n",
       "1  ['memeqbeceq', 'gy', 'sangegatel', 'yh', 'tete...   \n",
       "2  ['pertama', 'kali', 'denger', 'lagunya', 'enk'...   \n",
       "3  ['astajim', 'ini', 'pasti', 'yang', 'kasih', '...   \n",
       "4  ['beda', 'kalau', 'disini', 'kalau', 'komplain...   \n",
       "\n",
       "                                  stopword_tokenized  \\\n",
       "0  ['cm', 'spanduk', 'prof', 'video', 'orasi', 'b...   \n",
       "1  ['memeqbeceq', 'gy', 'sangegatel', 'yh', 'tete...   \n",
       "2  ['kali', 'denger', 'lagunya', 'enk', 'banget',...   \n",
       "3  ['astajim', 'kasih', 'penghargaan', 'sri', 'an...   \n",
       "4  ['beda', 'komplain', 'lgs', 'bully', 'ama', 'q...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  cm spanduk prof video orasi buku dll udh sngat...  \n",
       "1  memeqbeceq gy sangegatel yh tetekmemeky drnjng...  \n",
       "2  kali denger lagunya enk banget in pngn praktek...  \n",
       "3  astajim kasih penghargaan sri antek aseng wahy...  \n",
       "4  beda komplain lgs bully ama quotgenkquot kl fr...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil teks dan label\n",
    "texts = df['lemmatized_text'].astype(str).values\n",
    "labels = df[['pornografi', 'sara', 'radikalisme', 'pencemaran_nama_baik']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi teks\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "maxlen = 100\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowvecortizer = CountVectorizer()\n",
    "bow_vector = bowvecortizer.fit_transform(df['lemmatized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bow = bow_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.1, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (693, 100)\n",
      "y_train shape: (693, 4)\n",
      "X_test shape: (77, 100)\n",
      "y_test shape: (77, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(4, activation='sigmoid'))  # 4 adalah jumlah label\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=5000, output_dim=128))  # input_dim adalah ukuran vocabulary, output_dim adalah dimensi embedding\n",
    "# model.add(LSTM(128, return_sequences=True))  # return_sequences=True agar bisa menambahkan LSTM lain\n",
    "# model.add(Dropout(0.5))  # Dropout untuk mencegah overfitting\n",
    "# model.add(LSTM(64))  # LSTM kedua dengan ukuran layer 64\n",
    "# model.add(Dense(4, activation='sigmoid'))  # 4 adalah jumlah label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Membuat model LSTM\n",
    "# model = Sequential()\n",
    "# model.add(InputLayer(shape=(max_len,)))\n",
    "# model.add(Dense(128, activation='relu'))  # Mengganti Embedding dengan Dense layer\n",
    "# model.add(LSTM(128, return_sequences=False))\n",
    "# model.add(Dense(4, activation='sigmoid'))  # 4 output untuk 4 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompilasi model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.2204 - loss: 0.6267 - val_accuracy: 0.1688 - val_loss: 0.5154\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.1345 - loss: 0.4864 - val_accuracy: 0.1688 - val_loss: 0.5181\n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.1905 - loss: 0.4693 - val_accuracy: 0.5584 - val_loss: 0.4729\n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.4774 - loss: 0.4112 - val_accuracy: 0.4675 - val_loss: 0.3440\n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.4961 - loss: 0.2537 - val_accuracy: 0.5065 - val_loss: 0.3271\n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.5196 - loss: 0.1767 - val_accuracy: 0.5844 - val_loss: 0.3532\n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 0.6021 - loss: 0.1124 - val_accuracy: 0.5065 - val_loss: 0.3524\n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.6280 - loss: 0.0771 - val_accuracy: 0.5584 - val_loss: 0.3762\n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.6193 - loss: 0.0591 - val_accuracy: 0.5844 - val_loss: 0.3876\n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7246 - loss: 0.0354 - val_accuracy: 0.5974 - val_loss: 0.3672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1acca4a6b90>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Melatih model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.51      0.66        51\n",
      "           1       0.62      0.62      0.62         8\n",
      "           2       0.42      1.00      0.59         5\n",
      "           3       0.34      0.77      0.48        13\n",
      "\n",
      "    accuracy                           0.60        77\n",
      "   macro avg       0.58      0.73      0.59        77\n",
      "weighted avg       0.77      0.60      0.62        77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prediksi probabilitas untuk data uji\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Ubah probabilitas menjadi label kelas\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# Tampilkan classification report\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "[[0.84180343 0.00194653 0.00116207 0.00560451]]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\"ih memek kamu bau banget aku ga suka deh, tapi gapapa jadi pengen aku entot\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_padded_sequences = pad_sequences(new_sequences, maxlen=maxlen)\n",
    "predictions = model.predict(new_padded_sequences)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
